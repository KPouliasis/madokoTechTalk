[INCLUDE=presentation]
Title         : Obtaining and Organizing Big Data 
Sub Title     : bash, postgres and  pg-pool
Author        : Konstantinos Pouliasis
Affiliation   : Fullstack Academy
Email         : konstantinos.pouliasis@gmail.com
Reveal Theme  : sky
Beamer Theme  : singapore

[TITLE]

# FLIGHT TICKET DATASET 


* [Research and Innovative Technology Administration][rita]:
  Provides CSV files with historical data on flight tickets from the last 15 years organized quarterly
 * Each quarter entries around 7 GB ~ 30 GB per year
 * A total of more than 700 million records
[rita]:https://www.rita.dot.gov/bts/sites/rita.dot.gov.bts/files/subject_areas/airline_information/index.html



~ Slide 
``` javascript
200418,2004115,1,2004,1,10140,1014001,"ABQ","New Mexico",10397,30397,"ATL", ...
200418,2004116,2,2004,1,10397,1039703,"ATL","Georgia",10140,1014001,30140,"ABQ", 
200419,2004117,1,2004,1,10140,1014001,"ABQ","New Mexico",10693,1069301,30693,"BNA"
2004110,2004118,1,2004,1,10140,1014001,"ABQ","New Mexico",10693,1069301,30693,"BNA"
2004110,2004119,2,2004,1,10693,1069301,"BNA","Tennessee",10140,1014001,30140,"ABQ"
```
~


## Tools to peek them (UNIX rules!)
* Write a script that reads from a buffer five lines at a time 
* Turn it to an executable with **chmod + x **


``` javascript
#!/bin/bash
# Create new file handle 5
exec 5<  $1


# Now you can use "<&5" to read from this file
while   read line1 <&5 ; do
<!-- begin merge (remove this line to resolve the conflict) -->
~ Begin Remote
         read line2 <&5;
         read line3 <&5;
         read line4 <&5;
	       read line5 <&5;
         echo "$line1";
	       echo "$line2" ;
	       echo "$line3" ;
	       echo "$line4";
         echo "$line5"; 
	       read stdin;
~ End Remote
        read line2 <&5
        read line3 <&5
        read line4 <&5
	     read line5 <&5
        echo "$line1"
	      echo "$line2" 
	      echo "$line3"
	      echo  "$line4" 
       echo "$line5" 
	      read stdin
<!-- end merge -->
done

# Close file handle 5
exec 5<&-
```

## CURL THE ZIPS 

![Screenshot from 2017-03-15 02-18-53]

[Screenshot from 2017-03-15 02-18-53]: images/Screenshot-from-2017-03-15-02-18-53.png "Screenshot from 2017-03-15 02-18-53" { width:auto; max-width:90% }

##CURL THE ZIPS
``` javascript
curl 'https://[URL]/DownLoad_Table.asp?Table_ID=272&Has_Group=0&Is_Zipped=0' 

-H 'Pragma: 
   no-cache' 
-H 'Origin: https://www.transtats.bts.gov' 

 ... MORE HEADERS!!!

-H 'Referer: 
    URL/DL_SelectFields.asp?Table_ID=272&DB_Short_Name=Origin%20and%20Destination%20Survey' 

-H 'Cookie: BLA BLA BLA'


-H 'Connection: keep-alive' 
--data 'UserTableName=DB1BTicket&DBShortName=Origin_and_Destination_Survey&
        RawDataTable=T_DB1B_TICKET&sqlstr=+SELECTITIN_ID%2CCOUPONS%2CYEAR%2
        CQUARTER bla bla+
        FROM++T_DB1B_TICKET+
        WHERE+Quarter+%3D1+AND+YEAR%3D2016&varlist=ORIGIN_AIRPORT_ID%2
        BLA BLA BLA
        &VarName=ORIGIN_AIRPORT_ID&VarDesc=OriginAirportID&VarType=Num
        &VarName=ORIGIN_AIRPORT_SEQ_ID
      ...

        &VarType=Num&VarDesc=ItinFare&VarType=Num&VarDesc=Distance' --compressed
```
~ Slide
Run it and see the response
``` javascript
HTTP/1.1 302 Object moved
Cache-Control: private
Content-Length: 196
Content-Type: text/html
Location: https://transtats.bts.gov/ftproot/TranStatsData/715216664_T_DB1B_TICKET.zip
Server: Microsoft-IIS/8.5
X-Powered-By: ASP.NET
Date: Mon, 13 Mar 2017 04:12:57 GMT
Strict-Transport-Security: max-age=31536000 ; includeSubDomains
```
~

##Automate (curl -L on  repeat)

```javascript
dir=$(dirname $(dirname $(readlink -f "$0")))
for i in `seq 2004 2015`; do
  for j in `seq 1 4`; do
     curl  -L HEADERS , COOKIE, 
     --data 
     "AS BEFORE WITH  ... T_DB1B_TICKET+WHERE+Quarter+%3D${j}+AND+YEAR%3D${i} ..."
     --compressed -o "$dir/Y${i}_Q${j}/ticket${i}_Q${j}.zip"


  done;
done
```
## UNZIP, SED, REPEAT
```javascript
DIR=$(dirname $(dirname $(readlink -f "$0")));

echo $DIR;
for i in `seq 1 4`; do
    for j in `seq 2004 2015`; do
          unzip -p "$DIR/Y${j}_Q${i}/ticket*.zip" > 
                   "$DIR/Y${j}_Q${i}/ticket${j}_Q${i}.csv" ;
          sed '1d' "$DIR/Y${j}_Q${i}/ticket${j}_Q${i}.csv" > 
                    "$DIR/Y${j}_Q${i}/ticket${j}_Q${i}_Tail.csv"; 
          sed -i 's/,*$//g' "$DIR/Y${j}_Q${i}/ticket${j}_Q${i}_Tail.csv"
    done;
done
```


## PLAY WITH R

![passengers-2016q1]

[passengers-2016q1]: images/passengers-2016q1.png "passengers-2016q1" { height:80%; width:auto; max-width:90% }


# ORGANIZE IN A POSTGRES DB
## DB DESIGN  
* One big table for all tickets is not a great idea -indexes become very large
* Data are partitioned (seasonality)
* use postgres partitioning

##DB DESIGN
Create router table (in psql or using a pg connection library)
```javascript
CREATE TABLE  IF NOT EXISTS market_ticket (
      ITIN_ID         char(20),
      MKT_ID            char(23) PRIMARY KEY,
      MARKET_COUPONS	    int,
      ITIN_YEAR	    int,
      ITIN_QUARTER         smallint, CHECK(itin_quarter > 0 and itin_quarter< 5),
      ORIGIN_AIRPORT_ID        int,
      ORIGIN_AIRPORT_SEQ_ID        int,
      ... ... 
      PASSENGERS     int,
      market_fare    float,
      DISTANCE_GROUP smallint, check (distance_group > 0 and distance_group < 26),
      MARKET_MILES_FLOWN float
```
##DB DESIGN
Partition the main table per quarter. I.e. tables that inherit from that base 
```javascript
  for (let year = 2004; year < 2016; year++){
      for (let quarter = 1; quarter < 5; quarter++){
        yield client1.query(
          `CREATE TABLE IF NOT EXISTS 
            MARKET_QUARTER_TICKET_Y${year}_Q${quarter}
             (CHECK (itin_year = ${year} AND itin_quarter = ${quarter})) 
           INHERITS (market_ticket);)
          //ADD ANY INDEX YOU WANT PER PARTITION!
      }
    }
```
##MAKE BASE TABLE VIRTUAL
 * Ensure that each insertion in MARKET_TICKET ends up in a MARKET_QUARTER_TICKET
with a trigger. 

Read _new_ as a kind of _this_

```javascript
CREATE OR REPLACE FUNCTION market_ticket_insert_trigger()
    RETURNS trigger AS $$
    begin
    execute format ('INSERT INTO market_quarter_ticket_y%s_q%s
    VALUES (%L, %L, %L, %L, %L, %L, %L, %L, %L, %L, %L,%L, %L,%L, %L, %L, %L,
    %L, %L, %L, %L, %L, %L, %L, %L, %L, %L,%L, %L,%L);',
    new.itin_year, new.itin_quarter, new.itin_id,
    new.MKT_ID, new.MARKET_COUPONS, new.ITIN_YEAR,
    new.itin_QUARTER, new.ORIGIN_AIRPORT_ID,
    new.ORIGIN_AIRPORT_SEQ_ID,
    new.ORIGIN_CITY_MARKET_ID,
    new.origin,
    new.ORIGIN_STATE_NM,
    new.DEST_AIRPORT_ID,
    ...
    ...;
    return null;
    END;
```
##MAKE BASE TABLE VIRTUAL
* Override  insertion on the base table with the  trigger/ hook
```javascript
  yield client1.query(
    `CREATE TRIGGER insert_market_ticket_trigger 
     BEFORE INSERT on MARKET_TICKET FOR  EACH ROW EXECUTE PROCEDURE market_ticket_insert_trigger();
  ```

##CSV VALIDATION AND SEEDING (Sketch)
* Validations
 * Are the primary keys actually unique?
 * Is the `distance_group` column between 1 and 25?
 * Are boolean fields  boolean?
* Strategy
 * instead of parsing huge CSVs just use portgres!
 * create a temporary table 'RAW_TICKET_QY_YDDDD' for each quarter
 * validate as you insert from the CSV
 * copy the temporary into 'MARKET_TICKET' and do type casting to save space 


## INSERT DATA IN PARALLEL
* _pg-pool_ library is awesome!
* permits for multiple threads/ connections to postgres
* plays well with partitioning
* uses __co__-functions and __yield__ to make asynchronous code feel sequential
* You can write _for loops_ when _for loops_ feel right!

## CODE SAMPLE
```javascript
let quarterProcess = co.wrap(function *(year, quarter, client) {
  try {
    yield client.query(DROP_AND_CREATE_RAW_TICKET_MARKET_COUPON(year, quarter))
    yield client.query(SEED_RAW_TICKET_FROM_CSV(year, quarter))
    yield client.query(INSERT_INTO_MARKET_TICKET(year, quarter))
  } catch (e) {
      console.error(e.message, e.stack)
      throw Error(e)
  }
})

co(function *() {

  yield require('./createMarketTicketPartition')
  let clients = yield ([1, 2, 3, 4].map(() => co( function *() {
    return yield pool.connect()
  })))
  for (let year = 2007; year < 2016; year++) {
    yield [1, 2, 3, 4].map((quarter) => (
      quarterProcess(year, quarter, clients[quarter - 1])
    ))
  }
  yield clients.map((client) => client.release())
})
.then(() => {console.log('done'); return pool.end()})
.then(() => {console.log('Successfully closed Connection')})
.catch(err => {console.error(err.message, err.stack); pool.end()})
```
## What's wrong with sequelize
* Partitioning is new and ... good luck with documentation
* Doing __findAll__ in a 5GB table (e.g. copy Raw_ticket to market_ticket)
 is impossible - lists instead of streams
* Pagination with offset is unstable for large tables
* Workarounds are possible but  still very slow. 
* You can still generate models after the data processing phase
* Even automatically with __sequelize-auto__

## Thanks for your time :-)
